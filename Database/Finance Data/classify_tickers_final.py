#!/usr/bin/env python3
"""
Ph√¢n lo·∫°i m√£ c·ªï phi·∫øu th√†nh 2 nh√≥m:
1. M√£ chu·∫©n: Doanh nghi·ªáp th√¥ng th∆∞·ªùng (kh√¥ng ph·∫£i t√†i ch√≠nh/ng√¢n h√†ng)
2. M√£ ƒë·∫∑c bi·ªát: Ng√¢n h√†ng v√† to√†n b·ªô c√¥ng ty d·ªãch v·ª• t√†i ch√≠nh

Sau ƒë√≥ t√°ch d·ªØ li·ªáu th√†nh 2 file parquet ri√™ng bi·ªát.
"""

import pandas as pd
import json
import os
from datetime import datetime

def classify_and_split_data():
    """Ph√¢n lo·∫°i m√£ v√† t√°ch d·ªØ li·ªáu th√†nh 2 file parquet"""
    
    print("üìä PH√ÇN LO·∫†I V√Ä T√ÅCH D·ªÆ LI·ªÜU C·ªî PHI·∫æU")
    print("="*70)
    
    # Load data
    print("\n1. ƒêang load d·ªØ li·ªáu...")
    df = pd.read_parquet('Database/Buu_clean_ver2.parquet')
    
    # Get unique tickers with their industry
    ticker_industry = df.groupby('SECURITY_CODE')['ICB_L2'].first().reset_index()
    ticker_industry.columns = ['Ticker', 'Industry_ICB_L2']
    
    print(f"   ‚Ä¢ T·ªïng s·ªë m√£: {len(ticker_industry)}")
    
    # Separate by industry first
    # All financial services and banks go to special list
    financial_industries = ['Ng√¢n h√†ng', 'D·ªãch v·ª• t√†i ch√≠nh']
    
    # Get financial tickers
    special_df = ticker_industry[ticker_industry['Industry_ICB_L2'].isin(financial_industries)]
    standard_df = ticker_industry[~ticker_industry['Industry_ICB_L2'].isin(financial_industries)]
    
    # Sort by ticker
    standard_df = standard_df.sort_values('Ticker').reset_index(drop=True)
    special_df = special_df.sort_values('Ticker').reset_index(drop=True)
    
    print("\n2. K·∫øt qu·∫£ ph√¢n lo·∫°i:")
    print(f"   ‚úÖ M√£ chu·∫©n (kh√¥ng ph·∫£i t√†i ch√≠nh): {len(standard_df)} m√£")
    print(f"   üè¶ M√£ ƒë·∫∑c bi·ªát (ng√¢n h√†ng + d·ªãch v·ª• t√†i ch√≠nh): {len(special_df)} m√£")
    
    # Display industry breakdown
    print("\n3. Ph√¢n b·ªë theo ng√†nh:")
    print("\n   üìä M√£ chu·∫©n:")
    standard_industries = standard_df['Industry_ICB_L2'].value_counts()
    for industry, count in standard_industries.head(10).items():
        print(f"      ‚Ä¢ {industry}: {count} m√£")
    if len(standard_industries) > 10:
        print(f"      ... v√† {len(standard_industries)-10} ng√†nh kh√°c")
    
    print("\n   üè¶ M√£ ƒë·∫∑c bi·ªát:")
    special_industries = special_df['Industry_ICB_L2'].value_counts()
    for industry, count in special_industries.items():
        print(f"      ‚Ä¢ {industry}: {count} m√£")
    
    # Show all financial service companies
    print("\n   Danh s√°ch m√£ d·ªãch v·ª• t√†i ch√≠nh:")
    financial_services = special_df[special_df['Industry_ICB_L2'] == 'D·ªãch v·ª• t√†i ch√≠nh']['Ticker'].tolist()
    for i in range(0, len(financial_services), 10):
        group = financial_services[i:i+10]
        print(f"      {', '.join(group)}")
    
    # Save CSV files
    print("\n4. L∆∞u file CSV...")
    standard_df.to_csv('Database/standard_tickers_with_industry.csv', index=False, encoding='utf-8')
    print(f"   ‚úÖ Database/standard_tickers_with_industry.csv ({len(standard_df)} m√£)")
    
    special_df.to_csv('Database/special_tickers_with_industry.csv', index=False, encoding='utf-8')
    print(f"   ‚úÖ Database/special_tickers_with_industry.csv ({len(special_df)} m√£)")
    
    # Split parquet data
    print("\n5. T√°ch d·ªØ li·ªáu parquet...")
    
    standard_tickers = standard_df['Ticker'].tolist()
    special_tickers = special_df['Ticker'].tolist()
    
    # Filter data
    standard_data = df[df['SECURITY_CODE'].isin(standard_tickers)]
    special_data = df[df['SECURITY_CODE'].isin(special_tickers)]
    
    print(f"   ‚Ä¢ D·ªØ li·ªáu m√£ chu·∫©n: {len(standard_data):,} d√≤ng")
    print(f"   ‚Ä¢ D·ªØ li·ªáu m√£ ƒë·∫∑c bi·ªát: {len(special_data):,} d√≤ng")
    
    # Verify coverage
    total_rows = len(df)
    covered_rows = len(standard_data) + len(special_data)
    coverage = (covered_rows / total_rows) * 100
    print(f"   ‚Ä¢ ƒê·ªô ph·ªß: {coverage:.2f}% ({covered_rows:,}/{total_rows:,} d√≤ng)")
    
    # Save parquet files
    print("\n6. L∆∞u file parquet...")
    
    standard_data.to_parquet('Database/standard_data.parquet', index=False, compression='snappy')
    standard_size = os.path.getsize('Database/standard_data.parquet') / 1024 / 1024
    print(f"   ‚úÖ Database/standard_data.parquet ({standard_size:.2f} MB)")
    
    special_data.to_parquet('Database/special_data.parquet', index=False, compression='snappy')
    special_size = os.path.getsize('Database/special_data.parquet') / 1024 / 1024
    print(f"   ‚úÖ Database/special_data.parquet ({special_size:.2f} MB)")
    
    # Analyze metric types
    print("\n7. Ph√¢n t√≠ch metric codes:")
    
    # Standard metrics
    standard_metrics = standard_data['METRIC_CODE'].value_counts()
    standard_metric_types = set()
    for metric in standard_metrics.index[:100]:  # Check top 100
        if pd.notna(metric) and '_' in metric:
            standard_metric_types.add(metric.split('_')[0])
    
    print(f"\n   üìä M√£ chu·∫©n s·ª≠ d·ª•ng metrics: {', '.join(sorted(standard_metric_types))}")
    
    # Special metrics
    special_metrics = special_data['METRIC_CODE'].value_counts()
    special_metric_types = set()
    for metric in special_metrics.index[:100]:  # Check top 100
        if pd.notna(metric) and '_' in metric:
            special_metric_types.add(metric.split('_')[0])
    
    print(f"   üè¶ M√£ ƒë·∫∑c bi·ªát s·ª≠ d·ª•ng metrics: {', '.join(sorted(special_metric_types))}")
    
    # Create summary JSON
    summary = {
        'classification_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'standard': {
            'count': len(standard_df),
            'data_rows': len(standard_data),
            'size_mb': round(standard_size, 2),
            'metric_types': sorted(list(standard_metric_types)),
            'industries': standard_df['Industry_ICB_L2'].value_counts().to_dict()
        },
        'special': {
            'count': len(special_df),
            'data_rows': len(special_data),
            'size_mb': round(special_size, 2),
            'metric_types': sorted(list(special_metric_types)),
            'industries': special_df['Industry_ICB_L2'].value_counts().to_dict(),
            'tickers': special_df.to_dict('records')
        },
        'coverage': {
            'total_rows': total_rows,
            'covered_rows': covered_rows,
            'percentage': round(coverage, 2)
        }
    }
    
    with open('Database/ticker_classification_summary.json', 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\n   ‚úÖ Database/ticker_classification_summary.json")
    
    print("\n" + "="*70)
    print("üìå T√ìM T·∫ÆT:")
    print("="*70)
    print(f"‚Ä¢ M√£ chu·∫©n: {len(standard_df)} m√£ (doanh nghi·ªáp th√¥ng th∆∞·ªùng)")
    print(f"‚Ä¢ M√£ ƒë·∫∑c bi·ªát: {len(special_df)} m√£ (ng√¢n h√†ng + d·ªãch v·ª• t√†i ch√≠nh)")
    print(f"‚Ä¢ File d·ªØ li·ªáu:")
    print(f"  - standard_data.parquet: {standard_size:.2f} MB")
    print(f"  - special_data.parquet: {special_size:.2f} MB")
    print("\n‚úÖ HO√ÄN TH√ÄNH!")
    
    return standard_df, special_df

if __name__ == "__main__":
    standard_df, special_df = classify_and_split_data()
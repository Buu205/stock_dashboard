#!/usr/bin/env python3
"""
Script c·∫≠p nh·∫≠t d·ªØ li·ªáu t·ª´ final_processed_data.parquet sang Buu_clean_ver2.parquet
Gi·ªØ nguy√™n danh s√°ch ticker hi·ªán c√≥ trong Buu_clean_ver2.parquet

Author: Stock Dashboard System
Date: 2024
"""

import pandas as pd
import numpy as np
from pathlib import Path
import logging
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Thi·∫øt l·∫≠p logging
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

class DataUpdater:
    """Class x·ª≠ l√Ω c·∫≠p nh·∫≠t d·ªØ li·ªáu t·ª´ final_processed_data sang Buu_clean_ver2"""
    
    def __init__(self):
        # ƒê∆∞·ªùng d·∫´n file c·ªë ƒë·ªãnh
        self.base_path = Path("/Users/buu_os/Documents/GitHub/stock_dashboard/Database/Full_database")
        self.input_file = self.base_path / "final_processed_data.parquet"
        self.output_file = self.base_path / "Buu_clean_ver2.parquet"
        self.summary_file = self.base_path / "filtered_tickers_summary.csv"
        
    def validate_files(self) -> bool:
        """Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt c√≥ t·ªìn t·∫°i kh√¥ng"""
        if not self.input_file.exists():
            logger.error(f"Kh√¥ng t√¨m th·∫•y file input: {self.input_file}")
            return False
        if not self.output_file.exists():
            logger.warning(f"File {self.output_file} ch∆∞a t·ªìn t·∫°i, s·∫Ω t·∫°o m·ªõi")
        return True
    
    def get_existing_tickers_and_mapping(self) -> tuple:
        """L·∫•y danh s√°ch ticker v√† mapping ph√¢n ng√†nh t·ª´ file hi·ªán t·∫°i"""
        if not self.output_file.exists():
            logger.warning("File Buu_clean_ver2.parquet ch∆∞a t·ªìn t·∫°i")
            return [], {}
        
        logger.info(f"ƒêang load ticker t·ª´ {self.output_file.name}")
        existing_df = pd.read_parquet(self.output_file)
        
        # L·∫•y danh s√°ch ticker unique
        tickers = existing_df['SECURITY_CODE'].unique().tolist()
        logger.info(f"  ‚Ä¢ T√¨m th·∫•y {len(tickers)} ticker hi·ªán c√≥")
        
        # L·∫•y mapping ticker -> ICB_L2 (c√°ch t·ªëi ∆∞u)
        mapping = existing_df.groupby('SECURITY_CODE')['ICB_L2'].first().to_dict()
        
        # Lo·∫°i b·ªè gi√° tr·ªã NaN t·ª´ mapping
        mapping = {k: v for k, v in mapping.items() if pd.notna(v)}
        
        logger.info(f"  ‚Ä¢ T·∫°o mapping ph√¢n ng√†nh cho {len(mapping)} ticker")
        
        return tickers, mapping
    
    def process_data(self) -> pd.DataFrame:
        """X·ª≠ l√Ω d·ªØ li·ªáu ch√≠nh"""
        logger.info("\n" + "="*70)
        logger.info("B·∫ÆT ƒê·∫¶U C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU")
        logger.info("="*70)
        
        # 1. L·∫•y danh s√°ch ticker v√† mapping t·ª´ file hi·ªán c√≥
        logger.info("\n1. L·∫§Y DANH S√ÅCH TICKER HI·ªÜN C√ì")
        valid_tickers, sector_mapping = self.get_existing_tickers_and_mapping()
        
        if not valid_tickers:
            logger.error("Kh√¥ng t√¨m th·∫•y ticker n√†o trong file hi·ªán t·∫°i!")
            raise ValueError("File Buu_clean_ver2.parquet kh√¥ng t·ªìn t·∫°i ho·∫∑c kh√¥ng c√≥ d·ªØ li·ªáu")
        
        logger.info(f"  ‚Ä¢ S·ª≠ d·ª•ng {len(valid_tickers)} ticker t·ª´ file hi·ªán c√≥")
        logger.info(f"  ‚Ä¢ Mapping ph√¢n ng√†nh: {len(sector_mapping)} ticker")
        
        # 2. Load d·ªØ li·ªáu m·ªõi t·ª´ final_processed_data
        logger.info("\n2. LOAD D·ªÆ LI·ªÜU M·ªöI")
        logger.info(f"ƒêang load {self.input_file.name}...")
        main_df = pd.read_parquet(self.input_file)
        logger.info(f"  ‚Ä¢ Loaded {len(main_df):,} records")
        logger.info(f"  ‚Ä¢ Columns: {main_df.columns.tolist()}")
        
        # 3. L·ªçc theo danh s√°ch ticker hi·ªán c√≥
        logger.info("\n3. L·ªåC D·ªÆ LI·ªÜU THEO TICKER HI·ªÜN C√ì")
        initial_count = len(main_df)
        
        # Chu·∫©n h√≥a ticker trong main_df
        main_df['SECURITY_CODE'] = main_df['SECURITY_CODE'].str.upper().str.strip()
        
        # L·ªçc ch·ªâ gi·ªØ ticker hi·ªán c√≥
        filtered_df = main_df[main_df['SECURITY_CODE'].isin(valid_tickers)].copy()
        final_count = len(filtered_df)
        
        logger.info(f"  ‚Ä¢ L·ªçc t·ª´ {initial_count:,} xu·ªëng {final_count:,} records")
        logger.info(f"  ‚Ä¢ T·ª∑ l·ªá gi·ªØ l·∫°i: {(final_count/initial_count)*100:.1f}%")
        
        # Ki·ªÉm tra ticker n√†o kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi
        tickers_with_data = set(filtered_df['SECURITY_CODE'].unique())
        missing_tickers = set(valid_tickers) - tickers_with_data
        if missing_tickers:
            logger.warning(f"  ‚Ä¢ {len(missing_tickers)} ticker kh√¥ng c√≥ trong d·ªØ li·ªáu m·ªõi: {sorted(list(missing_tickers))[:10]}")
        
        # 4. Th√™m ph√¢n ng√†nh ICB_L2 t·ª´ mapping hi·ªán c√≥
        logger.info("\n4. GI·ªÆ NGUY√äN TH√îNG TIN PH√ÇN NG√ÄNH")
        filtered_df['ICB_L2'] = filtered_df['SECURITY_CODE'].map(sector_mapping)
        
        # Ki·ªÉm tra ticker kh√¥ng c√≥ ph√¢n ng√†nh (kh√¥ng n√™n x·∫£y ra v√¨ d√πng mapping c≈©)
        missing_sector = filtered_df[filtered_df['ICB_L2'].isna()]['SECURITY_CODE'].unique()
        if len(missing_sector) > 0:
            logger.warning(f"  ‚Ä¢ {len(missing_sector)} ticker kh√¥ng c√≥ th√¥ng tin ph√¢n ng√†nh")
            logger.warning(f"    {list(missing_sector[:10])}{'...' if len(missing_sector) > 10 else ''}")
            # ƒêi·ªÅn gi√° tr·ªã m·∫∑c ƒë·ªãnh
            filtered_df['ICB_L2'].fillna('Kh√¥ng x√°c ƒë·ªãnh', inplace=True)
        else:
            logger.info("  ‚Ä¢ T·∫•t c·∫£ ticker ƒë·ªÅu c√≥ th√¥ng tin ph√¢n ng√†nh")
        
        # 5. ƒê·∫£m b·∫£o th·ª© t·ª± c·ªôt gi·ªëng file g·ªëc (ICB_L2 ·ªü cu·ªëi)
        expected_columns = ['REPORT_DATE', 'YEAR', 'QUARTER', 'REPORTED_DATE', 'FREQ_CODE', 
                          'SECURITY_CODE', 'AUDITED', 'METRIC_CODE', 'M√¥ t·∫£', 'METRIC_VALUE', 'ICB_L2']
        
        # Ch·ªâ gi·ªØ c√°c c·ªôt c√≥ trong filtered_df
        final_columns = [col for col in expected_columns if col in filtered_df.columns]
        filtered_df = filtered_df[final_columns]
        
        logger.info(f"  ‚Ä¢ C·ªôt cu·ªëi c√πng: {filtered_df.columns.tolist()}")
        
        return filtered_df
    
    def save_output(self, df: pd.DataFrame):
        """L∆∞u file output v√† t·∫°o summary"""
        # L∆∞u file parquet
        logger.info("\n5. L∆ØU FILE K·∫æT QU·∫¢")
        df.to_parquet(self.output_file, index=False, compression='snappy')
        file_size = self.output_file.stat().st_size / (1024 * 1024)
        logger.info(f"  ‚úÖ ƒê√£ l∆∞u: {self.output_file.name} ({file_size:.2f} MB)")
        
        # T·∫°o summary
        summary_data = {
            'ticker': [],
            'sector': [],
            'record_count': [],
            'years': [],
            'quarters': []
        }
        
        for ticker in sorted(df['SECURITY_CODE'].unique()):
            ticker_data = df[df['SECURITY_CODE'] == ticker]
            summary_data['ticker'].append(ticker)
            summary_data['sector'].append(ticker_data['ICB_L2'].iloc[0])
            summary_data['record_count'].append(len(ticker_data))
            summary_data['years'].append(f"{ticker_data['YEAR'].min()}-{ticker_data['YEAR'].max()}")
            
            quarters = ticker_data[ticker_data['FREQ_CODE'] == 'Q']['QUARTER'].unique()
            summary_data['quarters'].append(len(quarters))
        
        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(self.summary_file, index=False)
        logger.info(f"  ‚úÖ ƒê√£ l∆∞u summary: {self.summary_file.name}")
    
    def print_statistics(self, df: pd.DataFrame):
        """In th·ªëng k√™ chi ti·∫øt"""
        logger.info("\n" + "="*70)
        logger.info("TH·ªêNG K√ä CHI TI·∫æT")
        logger.info("="*70)
        
        # Th·ªëng k√™ c∆° b·∫£n
        logger.info("\nüìä TH·ªêNG K√ä C∆† B·∫¢N:")
        logger.info(f"  ‚Ä¢ T·ªïng records: {len(df):,}")
        logger.info(f"  ‚Ä¢ S·ªë ticker: {df['SECURITY_CODE'].nunique()}")
        logger.info(f"  ‚Ä¢ S·ªë ph√¢n ng√†nh: {df['ICB_L2'].nunique()}")
        
        # Th·ªùi gian
        logger.info("\nüìÖ PH·∫†M VI TH·ªúI GIAN:")
        years = sorted(df['YEAR'].unique())
        logger.info(f"  ‚Ä¢ NƒÉm: {min(years)} - {max(years)}")
        
        # T√≠nh s·ªë qu√Ω c√≥ d·ªØ li·ªáu
        quarterly_data = df[df['FREQ_CODE'] == 'Q']
        if not quarterly_data.empty:
            quarters_by_year = quarterly_data.groupby('YEAR')['QUARTER'].nunique()
            logger.info(f"  ‚Ä¢ D·ªØ li·ªáu theo qu√Ω:")
            for year, q_count in quarters_by_year.items():
                logger.info(f"    - {year}: {q_count} qu√Ω")
        
        # Top ng√†nh
        logger.info("\nüè¢ TOP NG√ÄNH (theo s·ªë ticker):")
        sector_counts = df.groupby('ICB_L2')['SECURITY_CODE'].nunique().sort_values(ascending=False)
        for sector, count in sector_counts.head(10).items():
            logger.info(f"  ‚Ä¢ {sector}: {count} ticker")
        
        # Top ticker (theo s·ªë records)
        logger.info("\nüìà TOP TICKER (theo s·ªë records):")
        ticker_counts = df['SECURITY_CODE'].value_counts()
        for ticker, count in ticker_counts.head(10).items():
            sector = df[df['SECURITY_CODE'] == ticker]['ICB_L2'].iloc[0]
            logger.info(f"  ‚Ä¢ {ticker}: {count} records ({sector})")
    
    def run(self):
        """Ch·∫°y to√†n b·ªô quy tr√¨nh"""
        try:
            # Ki·ªÉm tra file
            if not self.validate_files():
                return False
            
            # X·ª≠ l√Ω d·ªØ li·ªáu
            processed_df = self.process_data()
            
            # L∆∞u k·∫øt qu·∫£
            self.save_output(processed_df)
            
            # In th·ªëng k√™
            self.print_statistics(processed_df)
            
            logger.info("\n" + "="*70)
            logger.info("‚úÖ C·∫¨P NH·∫¨T TH√ÄNH C√îNG!")
            logger.info("="*70)
            logger.info(f"\nFile output: {self.output_file}")
            logger.info(f"S·ª≠ d·ª•ng l·ªánh sau ƒë·ªÉ ki·ªÉm tra:")
            logger.info(f"  python3 -c \"import pandas as pd; df=pd.read_parquet('{self.output_file}'); print(df.info())\"")
            
            return True
            
        except Exception as e:
            logger.error(f"\n‚ùå L·ªñI: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

def main():
    """Main function"""
    print("\n" + "üöÄ "*20)
    print("SCRIPT C·∫¨P NH·∫¨T D·ªÆ LI·ªÜU BUU_CLEAN_VER2")
    print("üöÄ "*20)
    
    updater = DataUpdater()
    success = updater.run()
    
    if success:
        print("\n‚ú® Ho√†n th√†nh! D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t.")
    else:
        print("\n‚ùå C√≥ l·ªói x·∫£y ra. Vui l√≤ng ki·ªÉm tra log.")
    
    return success

if __name__ == "__main__":
    main()